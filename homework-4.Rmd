---
title: 'Homework 4: Bags, Forests, Boosts, oh my'
author: "Liang Lu"
date: "2/28/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r knitr-options, include = FALSE}
knitr::opts_chunk$set(fig.align="center",
                      warning = FALSE,
                      message = FALSE,
                      comment = NA)
```

### Load Packages

```{r libs, message = F, warning = F, include = F}
library(tidyverse)
library(broom)
library(glmnet)
library(caret)
library(ISLR)
library(janitor)
library(stringr)
library(rpart)
library(rpart.plot)
library(partykit)
library(randomForest)
library(MASS)
library(gbm)
theme_set(theme_bw())
```

## Problem 1

Problem 7 from Chapter 8 in the text. To be specific, please use a sequence of
`ntree` from 25 to 500 in steps of 25 and `mtry` from 3 to 9 for by 1. 

```{r}
set.seed(1234)
df <- tbl_df(Boston)

for (k in 1:20){
  inTraining <- createDataPartition(df$medv, p = .75, list = F)
  training <- df[inTraining, ]
  testing <- df[-inTraining, ]
  mtry <- c(3:9)
  ntree <- seq(25, 500, len = 20)
  results <- tibble(trial = rep(NA, 140),
  mtry = rep(NA, 140),
  ntree = rep(NA, 140),
  mse = rep(NA, 140)) 
  for(i in 1:7){
    cat(sprintf('Trial: %s, mtry: %s --- %s\n', k, mtry[i], Sys.time()))
    for(j in 1:20){ 
      rf_train <- randomForest(medv ~ .,
                               data = training,
                               mtry = mtry[i],
                               ntree = ntree[j])
      mse <- mean((predict(rf_train, newdata = testing) - testing$medv)^2)
      results[(i-1)*20 + j, ] <- c(k, mtry[i], ntree[j], mse)
    }
  }
  if(exists("results_total")){
  results_total <- bind_rows(results_total, results)
  }
  else(
  results_total <- results
  )
}
```


```{r }
results_total$mtry <- as.factor(results_total$mtry)
p <- ggplot(results, aes(x=ntree, y=mse, group=mtry, colour=mtry))
p + geom_line()
```

## Problem 2

Problem 8 from Chapter 8 in the text. Set your seed with 9823 and split into 
train/test using 50\% of your data in each split. In addition to 
parts (a) - (e), do the following

```{r}
set.seed(9823)
df <- tbl_df(Carseats)
```
a. Split the data into testing and training datasets.
```{r}
inTraining <- createDataPartition(df$Sales, p = .5, list = F)
training <- df[inTraining, ]
testing  <- df[-inTraining, ]
```
b. Fit a regression tree to the training dataset.

```{r}
sales_tree <- rpart::rpart(Sales ~ ., 
                      data = training,
                      control = rpart.control(minsplit = 50))
sales_tree
```

```{r}
prp(sales_tree)
```

```{r}
plot(as.party(sales_tree))
```
```{r}
test_preds <- predict(sales_tree, newdata = testing)
sales_test_df <- testing %>%
  mutate(y_hat = test_preds,
         sq_err = (y_hat - Sales)^2)
mean(sales_test_df$sq_err)
```

```{r}
sqrt(mean(sales_test_df$sq_err))
```
c. use cross-validation
```{r}
set.seed(1982)

rf_sales_cv <- train(Sales ~ ., 
                      data = training,
                      method = "rf",
                      ntree = 100,
                      importance = T,
                      tuneGrid = data.frame(mtry = 1:11))
rf_sales_cv
```

```{r}
plot(rf_sales_cv)
```

```{r}
p <- ggplot(data = rf_sales_cv$results,
            aes(x = mtry, y = RMSE))
p + geom_point() +
  geom_line()
```


```{r}
rf_sales_9 <- randomForest(Sales ~ ., 
                            data = training,
                            mtry = 9)
rf_sales_9
```

And the test MSE is:

```{r}
test_preds <- predict(rf_sales_9, newdata = testing)
sales_test_df <- sales_test_df %>%
  mutate(y_hat_rf_9 = test_preds,
         sq_err_rf_9 = (y_hat_rf_9 - Sales)^2)
mean(sales_test_df$sq_err_rf_9)
```

The test MSE droped from 5.43 to 3.04.

d. Use bagging to analyze the data.

```{r}
set.seed(10982)
bag_sales <- randomForest(Sales ~ ., data = training, mtry = 9)
bag_sales
```

Compute the test MSE:

```{r}
test_preds <- predict(bag_sales, newdata = testing)
sales_test_df <- testing %>%
  mutate(y_hat_bags = test_preds,
         sq_err_bags = (y_hat_bags - Sales)^2)
mean(sales_test_df$sq_err_bags)
```
```{r}
imp <- varImp(rf_sales_cv)$importance
rn <- row.names(imp)
imp_df <- data_frame(variable = rn, 
                     importance = imp$Overall) %>%
  arrange(desc(-importance)) %>%
  mutate(variable = factor(variable, variable))
p <- ggplot(data = imp_df,
            aes(variable, importance))
p + geom_col(fill = "#6e0000") +
  coord_flip()
```
ShelveLocGood is the most important factor and price the next. 

1. Fit a gradient-boosted tree to the training data and report the estimated 
test MSE. 
```{r}
set.seed(99)
grid <- expand.grid(interaction.depth = c(1, 3), 
                    n.trees = seq(0, 2000, by = 100),
                    shrinkage = c(.01, 0.001),
                    n.minobsinnode = 10)
trainControl <- trainControl(method = "cv", number = 5)
gbm_boston <- train(Sales ~ ., 
                    data = training, 
                    distribution = "gaussian", 
                    method = "gbm",
                    trControl = trainControl, 
                    tuneGrid = grid,
                    verbose = FALSE)
gbm_boston
```
```{r}
test_preds <- predict(gbm_boston, newdata = testing)
sales_test_df <- sales_test_df %>%
  mutate(y_hat_gbm = test_preds,
         sq_err_gbm = (y_hat_gbm - Sales)^2)
mean(sales_test_df$sq_err_gbm)
```


2. Fit a multiple regression model to the training data and report the 
estimated test MSE
```{r}
sales_lm <- lm(Sales ~ ., data = df)
summary(sales_lm)
```
```{r}
test_preds <- predict(sales_lm, newdata = testing)
sales_test_df <- sales_test_df %>%
  mutate(y_hat_gbm = test_preds,
         sq_err_gbm = (y_hat_gbm - Sales)^2)
mean(sales_test_df$sq_err_gbm)
```
3. Summarize your results. 
